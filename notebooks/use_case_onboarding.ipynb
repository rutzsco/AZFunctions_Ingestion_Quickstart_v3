{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case Onboarding\n",
    "\n",
    "Sample notebook demonstrating how to utilize the deployed functions to:\n",
    "- Create a new index\n",
    "- List all files within a target Azure Storage container\n",
    "- Trigger ingestion and indexing of that collection of files into the target storage index\n",
    "- Retrieve a list of all chunks which have been added into the target index\n",
    "\n",
    "Note: Before running this notebook, you will need to have deployed the Azure Durable Functions project to an Azure Function App environment as well as all associated resources (Azure AI Search, Azure Storage, Azure OpenAI, Azure Document Intelligence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variable Configuration\n",
    "\n",
    "Create a `.env` file in your working directory with the following key-value pairs. We will load these into code using the `python-dotenv` library. \n",
    "\n",
    "| Variable       | Description                                                                                 |  \n",
    "|----------------|---------------------------------------------------------------------------------------------|  \n",
    "| FUNCTION_URI   | The primary URI endpoint for accessing your deployed Azure Function App.                    |  \n",
    "| FUNCTION_KEY   | The default host key used for authenticating and securing access to your Azure Function App. | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "function_uri = os.getenv(\"FUNCTION_URI\")\n",
    "function_key = os.getenv(\"FUNCTION_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Execution Variables\n",
    "\n",
    "Below, create a stem name for a new Azure AI Search Index and fields that will be used in index creation. Further, define Azure Storage containers which contain source documents and which will contain extracted chunks, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Creation Settings\n",
    "index_stem_name = 'test-index'\n",
    "fields = {\n",
    "    \"content\": \"string\", \"pagenumber\": \"int\", \"sourcefile\": \"string\", \n",
    "    \"sourcefilepath\": \"string\",\"sourcepage\": \"string\", \"category\": \"string\"\n",
    "}\n",
    "embedding_dimensions = 3072 # Update according to the embedding model used (3072 for text-embedding-large-003; 1536 for text-embedding-ada-002)\n",
    "\n",
    "# Data Source Settings\n",
    "source_container = 'a-test-source'\n",
    "extract_container = 'a-test-extract'\n",
    "\n",
    "# Ingestion Settings\n",
    "automatically_delete = False\n",
    "analyze_images = True\n",
    "overlapping_chunks = False\n",
    "chunk_size = 800\n",
    "chunk_overlap = 200\n",
    "cosmos_logging = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Create New Index\n",
    "\n",
    "Trigger the `create_new_index` function and store the created index name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_uri = f\"{function_uri}/api/create_new_index?code={function_key}\"\n",
    "create_index_payload = {\n",
    "    \"index_stem_name\": index_stem_name,\n",
    "    \"fields\": fields,\n",
    "    \"dimensions\": embedding_dimensions\n",
    "}\n",
    "\n",
    "response = requests.post(create_index_uri, json=create_index_payload)\n",
    "index_name = response.text\n",
    "print(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - List Files in Source Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files_uri = f\"{function_uri}/api/list_files_in_container?code={function_key}\"\n",
    "\n",
    "response = requests.post(list_files_uri, json={\"container\": source_container})\n",
    "files = response.json()\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Trigger Ingestion for all Files and Await Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_processing(blob_name, index_name):\n",
    "    body_template = {\n",
    "        'source_container': source_container,\n",
    "        'extract_container': extract_container,\n",
    "        'prefix_path':'',\n",
    "        'index_name': index_name,\n",
    "        'automatically_delete': automatically_delete,\n",
    "        'analyze_images': analyze_images,\n",
    "        'overlapping_chunks': overlapping_chunks,\n",
    "        'chunk_size': chunk_size,\n",
    "        'chunk_overlap': chunk_overlap,\n",
    "        'cosmos_logging': cosmos_logging\n",
    "    }\n",
    "\n",
    "    body = body_template.copy()\n",
    "    body['prefix_path'] = blob_name\n",
    "    function_uri = f'{os.environ[\"FUNCTION_URI\"]}/api/orchestrators/pdf_orchestrator?code={os.environ[\"FUNCTION_KEY\"]}'\n",
    "    response = requests.post(function_uri, json=(body))\n",
    "    return response.json()['statusQueryGetUri']\n",
    "\n",
    "def get_status(status_uri):\n",
    "    response = requests.get(status_uri)\n",
    "    status = response.json()['runtimeStatus']\n",
    "    error = ''\n",
    "    if status == 'Failed':\n",
    "        error = response.json()['output']\n",
    "    return status, error\n",
    "\n",
    "\n",
    "# Submit all files for ingestion\n",
    "tracking_dict = {}\n",
    "\n",
    "for blob in files:\n",
    "    blob_name = blob\n",
    "    tracking_dict[blob_name] = {}\n",
    "    status_uri = start_processing(blob_name, index_name)\n",
    "    tracking_dict[blob_name]['status_uri'] = status_uri\n",
    "    tracking_dict[blob_name]['submitted'] = True\n",
    "    tracking_dict[blob_name]['completed'] = False\n",
    "    status, error = get_status(status_uri)\n",
    "    tracking_dict[blob_name]['error'] = error\n",
    "    tracking_dict[blob_name]['status'] = status\n",
    "\n",
    "print(f'Submitted {str(len(tracking_dict))} blobs for processing')\n",
    "\n",
    "while True:\n",
    "    all_complete = True\n",
    "    total_complete = 0\n",
    "    for k,v in tracking_dict.items():\n",
    "        status, error = get_status(v['status_uri'])\n",
    "        v['status'] = status\n",
    "        v['error'] = error\n",
    "        if v['status'] == 'Completed':\n",
    "            v['completed'] = True\n",
    "            total_complete += 1\n",
    "        elif v['status'] == 'Failed': \n",
    "            # Logic to proceed forward WITHOUT retrying failed blobs\n",
    "            status, error = get_status(status_uri)\n",
    "            v['error'] = error\n",
    "            v['status'] = status\n",
    "            v['completed'] = True\n",
    "            total_complete +=1\n",
    "\n",
    "            # Logic to retry failed blobs\n",
    "            # status_uri = start_processing(k)\n",
    "            # v['status_uri'] = status_uri\n",
    "            # v['submitted'] = True\n",
    "            # v['completed'] = False\n",
    "        else:\n",
    "            all_complete = False\n",
    "    clear_output(wait=True)\n",
    "    print(f'Completed: {total_complete}/{len(tracking_dict)}')\n",
    "    display(pd.DataFrame(tracking_dict).T)\n",
    "    if all_complete:\n",
    "        break\n",
    "    time.sleep(10)\n",
    "\n",
    "clear_output(wait=True)\n",
    "print('All Blobs Processed')\n",
    "succeeded = len([k for k,v in tracking_dict.items() if v['status'] == 'Completed'])\n",
    "failed = len([k for k,v in tracking_dict.items() if v['status'] != 'Completed'])\n",
    "\n",
    "print(f'Succeeded: {succeeded}')\n",
    "succeeded_dict = {k: v for k, v in tracking_dict.items() if v['status'] == 'Completed'}\n",
    "display(pd.DataFrame(succeeded_dict).T)\n",
    "print()\n",
    "print(f'Failed: {failed}')\n",
    "failed_dict = {k: v for k, v in tracking_dict.items() if v['status'] != 'Completed'}\n",
    "display(pd.DataFrame(failed_dict).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.5 (Optional) - Attempt to Resubmit Failed Files\n",
    "\n",
    "Uncommend the logic below and execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in failed_dict.items():\n",
    "#     status_uri = start_processing(k)\n",
    "#     v['status_uri'] = status_uri\n",
    "#     v['submitted'] = True\n",
    "#     v['completed'] = False\n",
    "\n",
    "# while True:\n",
    "#     all_complete = True\n",
    "#     total_complete = 0\n",
    "#     for k,v in failed_dict.items():\n",
    "#         status, error = get_status(v['status_uri'])\n",
    "#         v['status'] = status\n",
    "#         v['error'] = error\n",
    "#         if v['status'] == 'Completed':\n",
    "#             v['completed'] = True\n",
    "#             total_complete += 1\n",
    "#         elif v['status'] == 'Failed': # Resubmit\n",
    "#             status_uri = start_processing(k)\n",
    "#             v['status_uri'] = status_uri\n",
    "#             v['submitted'] = True\n",
    "#             v['completed'] = False\n",
    "#             status, error = get_status(status_uri)\n",
    "#             v['error'] = error\n",
    "#             v['status'] = status\n",
    "#             v['completed'] = True\n",
    "#             total_complete +=1\n",
    "#         else:\n",
    "#             all_complete = False\n",
    "#     clear_output(wait=True)\n",
    "#     print(f'Completed: {total_complete}/{len(failed_dict)}')\n",
    "#     display(pd.DataFrame(failed_dict).T)\n",
    "#     if all_complete:\n",
    "#         break\n",
    "#     time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Run 'Sync Index' to Retrieve a List of all Indexed Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_index_uri = f\"{function_uri}/api/orchestrators/sync_index_orchestrator?code={function_key}\"\n",
    "response = requests.post(sync_index_uri, json={\"index_name\": index_name, \"extract_container\": extract_container})\n",
    "status_uri = response.json()['statusQueryGetUri']\n",
    "\n",
    "while True:\n",
    "    status, error = get_status(status_uri)\n",
    "    if status == 'Completed':\n",
    "        clear_output(wait=True)\n",
    "        break\n",
    "    clear_output(wait=True)\n",
    "    print(f'Status: {status}')\n",
    "    print(f'Error: {error}')\n",
    "    time.sleep(10)\n",
    "\n",
    "output = requests.get(status_uri)\n",
    "index_content = output.json()['output']['index_content']\n",
    "print(f'Indexed Content: {index_name}')\n",
    "pd.DataFrame(index_content)\n",
    "# Optional: save as CSV\n",
    "# pd.DataFrame(index_content).to_csv(f'{index_name}.csv', index=False)\n",
    "\n",
    "# Prints full list of indexed content\n",
    "# print(json.dumps(output.json()['output']['index_content'], indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
